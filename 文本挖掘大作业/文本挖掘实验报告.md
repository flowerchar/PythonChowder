## 文本挖掘任务

> 给定数据集1，完成如下任务：
> （1）从维基百科中下载合适的训练数据并训练词向量模型。
>
> （2）对数据集1（每行一个文本）， 分别抽取每个文本的top-10关键词抽取关键词及其权重，并归并所有文本的关键词及其权重，构建词云图。
>
> （3）基于训练好的词向量模型计算词汇之间的语义相似度，总结归纳这组关键词表达了哪些主题信息。
>
> 要求：
>
> ① 下载训练语料，实现数据预处理和模型训练，描述训练算法和实验步骤，并记录训练过程。
>
> ② 下载语料：Wikipedia中文语料库链接：https://dumps.wikimedia.org/zhwiki/
>
> ③ 要是自己训练词向量有难度，可以理论描述相关算法，不展示训练过程，从群文件中下载训练好的词向量模型
>
> ④ 描述关键词抽取所使用的算法和调用的代码包及关键，展示词云结果
>
> ⑤ 关键词抽取及权重显示理论及代码：https://zhuanlan.zhihu.com/p/406354320
>
> ⑥ 在线词云图绘制https://www.ciyunwenzi.com/
>
> ⑦ 描述计算词汇语义相似度的过程，并实现***\*相似度网络可视化。\****

## 一、开发环境准备

### 1.1 Python环境

由于Python版本过低会导致版本不兼容，过高会导致第三方模块更新不及时，因此本次实验报告采用的版本是windows下的Python 3.9.0，在[Python3.9.0](https://www.python.org/ftp/python/3.9.0/python-3.9.0rc2-amd64.exe)下载对应版本![image-20220616104939572](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616104939572.png)

### 1.2 依赖模块

- gensim：训练模型

- numpy：gensim底层依赖

- scipy：gensim底层依赖

- jieba：分词，计算关键字与TF_IDF

- wordcloud：绘制词云图

- os：管理文件夹

- pickle：序列化与反序列化文件

- logging：打印日志，方便调试


命令行键入 pip install -i https://pypi.tuna.tsinghua.edu.cn  模块名 

> 使用清华镜像可避免外网因素导致下载失败

![image-20220616105801293](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616105801293.png)

## 二、Wiki数据获取

###  2.1 wiki中文数据的下载

到wiki官网下载中文语料（[点击此处](https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2)），下载完成后得到一个名字为zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2的文件，大小为2.26GB

### 2.2 将XML格式的数据转为txt格式

#### 2.2.1 Python实现

编写Python程序将XML文件转换为txt格式，使用到了gensim.corpora中的WikiCorpus函数来处理维基百科的数据。python代码实现如下图所示，文件取名为1_process.py。

![image-20220616154047284](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616154047284.png)

#### 2.2.2 运行程序文件

在代码文件夹下运行cmd命令，即可得到转换后生成的文件wiki.zh.txt。

> python 1_process.py zhwiki-latest-pages-articles.xml.bz2   wiki.zh.txt

2. 使用一个多小时用Python读取.xml.bz2文件为文本文件，处理了428680篇文章

![image-20220616153615888](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616153615888.png)

#### 2.2.3 得到运行结果

最后生成的wiki.zh.txt文件大小为1.43GB

![image-20220616154213894](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616154213894.png)

## 三、Wiki数据预处理

### 3.1 中文繁体替换成简体

因为Wiki中文语料里包含了很多繁体字，需要转成简体字再进行处理，这里使用Opencc对文字进行转变。

#### 3.1.1 安装opencc

可以使用Python版本的opencc-python进行代码处理，但是wiki.zh.txt文件太大甚至都不能用记事本打开，但是直接使用opencc工具在命令行进行转换可以极大加快速度，这是Windows下的opencc工具[点击此处下载](https://github.com/BYVoid/OpenCC)

![image-20220616162525613](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616162525613.png)

#### 3.1.2 进行繁简字转换

在命令行键入

> opencc -i C:\Users\DELL\PycharmProjects\StageTwo\文本挖掘大作业\wiki.zh.txt -o C:\Users\DELL\PycharmProjects\StageTwo\文本挖掘大作业\wiki.zh.simp.txt -c t2s.json

在命令行状态下都花了很久才转换完毕

> 记录下这里有两个坑！！
>
> ​	1.下载好opencc工具后，需要将bin目录添加到环境变量
>
> ​	2.使用到配置文件t2s.json，需要在\OpenCC\build\share\下启动命令行，输入输出文件采用绝对路径，配置文件就在当前目录下，因此可以直接调用

![image-20220616162614851](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616162614851.png)

#### 3.1.3 查看结果

繁体字：

![image-20220616163550462](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616163550462.png)

经过opencc处理之后变成简体字：

![image-20220616163616752](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616163616752.png)

### 3.2 jieba分词

本报告中采用结巴分词对字体简化后的wiki中文语料数据集进行分词，在执行代码前需要安装jieba模块。由于此语料已经去除了标点符号，因此在分词程序中无需进行清洗操作，可直接分词。若是自己采集的数据还需进行标点符号去除和去除停用词的操作。 Python实现代码如下（分词处理1.43GB txt文件用了一个小时）：

![image-20220616172553255](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616172553255.png)

经过jieba分词后的wiki.zh.simp.seg.txt(1.76GB)：

![image-20220616172651542](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616172651542.png)



## 四、Word2Vec模型训练

### 4.1 训练模型
训练词向量模型，执行文件3_train_word2vec_model.py，代码如下：

![image-20220616180747681](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616180747681.png)

### 4.2 查看结果

运行结束之后产生四个文件，wiki.zh.text.model是建好的模型，wiki.zh.text.vector是词向量(训练一个多小时)

![image-20220616180830299](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616180830299.png)

多生成的文件

![image-20220616181224140](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616181224140.png)

## 五、计算权重并绘制词云图

### 5.1 分割文本

在词向量抽取数据集.txt中，每一行就是一个文本，一共80个文本，编写代码，分出来80个文本：

![image-20220616200809883](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616200809883.png)

### 5.2 计算TF_IDF

其中AllWords是80个文本的top10关键词，但是存放于字典里的键值对可能有重复，因此实际AllWords的长度小于800（629个），currentWord是当次文本的top10关键词和权重，代码如下：

![image-20220616201140950](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616201140950.png)

第7个文本的前十个关键字的tf_idf指标

![image-20220616201720358](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616201720358.png)



### 5.3 绘制词云图

绘制80个文本的关键字词云图（根据权重），代码如下：

![image-20220616201300470](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616201300470.png)

80个文本的top10关键字词云图：

![image-20220616201556909](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616201556909.png)

所有文本的关键词及其权重，构建词云图：

![image-20220616202844750](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616202844750.png)

### 5.4 序列化

AllWords是对整个词云图向量抽取数据集.txt的关键字，把这个字典序列化，代码如下：

![image-20220616201630344](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616201630344.png)

## 六、计算词汇语义相似度

### 6.1 反序列化文件

因为全的txt关键字保存在data.pickle中，所以需要读取出来

```python
with open('data.pickle', 'rb') as f:  # 读入时同样需要指定为读取字节流模式
    data = pickle.load(f)
    # data有629个
```

### 6.2 利用训练好的模型计算相似度

![image-20220616202352371](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616202352371.png)

运行结果：

![image-20220616202431905](%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220616202431905.png)

> 记录遇到的坑：
>
> ​	1.词语图想要展现中文，必须在WordCloud中制定一个中文字体
>
> ​	2.在Python2中是 model.most_similar(word)，Python3是 model.wv.most_similar(word)







